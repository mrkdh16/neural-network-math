{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068185be-5aca-4cf1-b625-a28dcb6d4f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01ab4ee-b15e-48f1-97be-d714f2454556",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m,n = data.shape # m test digits(rows), n pixels(columns)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data_dev = data[0:1000].T # transpose data matrix for ease of use, 784xm\n",
    "Y_dev = data_dev[0] # 1xm matrix with labels, indicates correct answer\n",
    "X_dev = data_dev[1:n] # 784xm matrix with image data, each column is one digit\n",
    "X_dev = X_dev / 255\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339719c8-4d71-44ec-8dcc-ac1fcf787355",
   "metadata": {},
   "source": [
    "### Remark.\n",
    "\n",
    "    The dataset is split into two parts in order to train the model on the larger part and test the model on the smaller one. This prevents the model from overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7f25ab-41b8-47d4-95b9-31f29e7e4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initWB(n_h1,n_h2):\n",
    "    # W1 = np.random.rand(n_h1, 784) # n_h1x784 matrix with random weights from [0,1) (uniformly chosen)\n",
    "    # b1 = np.random.rand(n_h1, 1) # n_h1x1 vector with random biases from [0,1)\n",
    "    # W2 = np.random.rand(n_h2, n_h1) # n_h2xn_h1\n",
    "    # b2 = np.random.rand(n_h2, 1) # n_h2x1\n",
    "    W1 = np.random.rand(n_h1, 784) - 0.5\n",
    "    b1 = np.random.rand(n_h1, 1) - 0.5\n",
    "    W2 = np.random.rand(n_h2, n_h1) - 0.5\n",
    "    b2 = np.random.rand(n_h2, 1) - 0.5\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fdc9d2-8a15-4666-8116-b6647be35cf7",
   "metadata": {},
   "source": [
    "### Observation.\n",
    "\n",
    "    Initializing the weights and biases in the range of [0.5,0.5) results in much faster learning than initializing them in the range of [0,1). \n",
    "\n",
    "### Thoughts.\n",
    "\n",
    "    This may have to do with the fact that weights and biases get blown up throught the iterations if they are always positive. Having negative values may help the gradient descent happen more smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b26548-209a-455c-b1ad-cf5f7f9d3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return (Z>0).astype(float)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "\n",
    "def feedForward(W1,b1,W2,b2,X):\n",
    "    Z1 = np.matmul(W1,X)+b1 # n_h1xm matrix of weighted sum of previous nodes \n",
    "    A1 = ReLU(Z1) # n_h1xm\n",
    "    Z2 = np.matmul(W2,A1)+b2 # n_h2xm \n",
    "    A2 = softmax(Z2) # n_h2xm\n",
    "    # A2 = ReLU(Z2)\n",
    "    return Z1,A1,Z2,A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7bf66-ba8d-4a35-b4a4-c68d433a7a66",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"diagrams/IMG_4953.png\" width=\"500\" alt=\"super simple NN\"/>\n",
    "</center>\n",
    "\n",
    "(IMG_4953)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eecb48-3344-46cf-9915-e4662c3d839f",
   "metadata": {},
   "source": [
    "### The Math.\n",
    "\n",
    "The following shows how feeding forward in neural networks works. $z$ denotes the weighted sum of the previous activations with respect to weights plus the bias. $a$ denotes the weighted sum $z$ ran through a non-linear activation function $\\sigma$ (this is important as to not have each activation simply be a linear combination of the inputs). $w$ denotes the weights between two activations and $b$ denotes the bias for each layer.\n",
    "\n",
    "$$z^{(L)}_j = \\sum_{n=1}^{m} a^{(L-1)}_n*w^{(L)}_{jn} + b^{(L)}$$\n",
    "$$a^{(L)}_j = \\sigma(z^{(L)}_j)$$\n",
    "\n",
    "The superscripts denote the layer the node is in (or, for the weights, what layer they are heading to) and the subscripts denote the index number within the layer. For example, $$z^{(L)}_j$$ denotes the weighted sum of the $j$th node in the $L$th layer. Additionally, $$w^{(L)}_{jn}$$ denotes the weight of the edge connecting the $n$th node in the $L-1$th layer to the $j$th node in the $L$th layer. The component-wise form can be condensed into a matrix-form equation which makes the notation and code much simpler.\n",
    "\n",
    "$$A^{(L)} = \\sigma(W^{(L)}A^{(L-1)}+B^{(L)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055fe08-51c9-4c50-9792-bf3d95f01fd2",
   "metadata": {},
   "source": [
    "### Observation.\n",
    "\n",
    "Using ReLU for the hidden layer and softmax for the output layer results in the fastest learning speed -- the difference is pretty dramatic with using the softmax resulting in accuracy almost 6 times that of using just the ReLU. Without the softmax, the activation function for the final layer effectively becomes the argmax function (since ReLU returns the input for any positive input) which simply returns the largest value in the output vector. \n",
    "\n",
    "### Thoughts.\n",
    "\n",
    "I think the process of multiplying the derivative of ReLU for delta2 in the process of backpropagation may result in a lot of zeros, which slows down gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1e2509-c2bf-4804-b8d4-5e1cf27be2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, 10))\n",
    "    for i in range(0,Y.size):\n",
    "        one_hot_Y[i,Y[i]]=1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "    \n",
    "def backProp(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    m = X.shape[1] # number of test digits\n",
    "    Y = one_hot(Y)\n",
    "    \n",
    "    delta2 = (A2-Y) # n_h2xm matrix\n",
    "    # delta2 = (A2-Y)*ReLU_deriv(Z2)\n",
    "    dW2 = np.matmul(delta2,A1.T)/m # n_h2xn_h1 matrix, matrix multiplying inherently involves summing up all test case errors which means division by m is necessary\n",
    "    db2 = np.sum(delta2,axis=1,keepdims=True)/m # n_h2x1 matrix\n",
    "    \n",
    "    delta1 = np.matmul(W2.T,delta2)*ReLU_deriv(Z1) # n_h1xm matrix\n",
    "    dW1 = np.matmul(delta1,X.T)/m # n_h1xm matrix\n",
    "    db1 = np.sum(delta1,axis=1,keepdims=True)/m # n_h1x1 matrix\n",
    "    \n",
    "    return dW1, dW2, db1, db2\n",
    "\n",
    "def updateWB(W1, W2, b1, b2, dW1, dW2, db1, db2, alpha):\n",
    "    W1 -= alpha*dW1\n",
    "    W2 -= alpha*dW2\n",
    "    b1 -= alpha*db1\n",
    "    b2 -= alpha*db2\n",
    "    return W1,b1,W2,b2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b879d2-55c9-46cf-9c6c-33d716cf8cd8",
   "metadata": {},
   "source": [
    "### Observation.\n",
    "\n",
    "The importance of paying attention to the dimensions of the matrices cannot be understated. When turning component-wise math into more compact matrix form notation, the easiest way to keep track of everything is to always keep the dimensions of the matrices in mind. This is especially important when transposes are necessary to make the dimensions match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0135f1e-52a6-4153-8d9f-c527fe8bb258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0) #returns an array of indices indicating the likliest digit by finding the largest number along each column\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradDesc(X,Y,alpha,iterations):\n",
    "    W1,b1,W2,b2 = initWB(10,10)\n",
    "    for i in range(iterations):\n",
    "        Z1,A1,Z2,A2 = feedForward(W1,b1,W2,b2,X)\n",
    "        dW1, dW2, db1, db2 = backProp(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1,b1,W2,b2 = updateWB(W1, W2, b1, b2, dW1, dW2, db1, db2, alpha)\n",
    "        # if(i%10==0):\n",
    "        #     print(\"interation: \",i)\n",
    "        #     predictions = get_predictions(A2)\n",
    "        #     print(get_accuracy(predictions, Y))\n",
    "        # if(i==iterations-1):\n",
    "        #     print(\"interation: \",i)\n",
    "        #     predictions = get_predictions(A2)\n",
    "        #     print(get_accuracy(predictions, Y))\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "def testWB(W1,b1,W2,b2,X,Y):\n",
    "    Z1,A1,Z2,A2 = feedForward(W1,b1,W2,b2,X)\n",
    "    predictions = get_predictions(A2)\n",
    "    print(\"accuracy for test data:\",get_accuracy(predictions, Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a53dd2-43f3-4200-bdc0-45bcb1db2d55",
   "metadata": {},
   "source": [
    "### Remark.\n",
    "\n",
    "The process of repeating the feedForward and backpropagation means the neural network \"learns\" as iterations increase. The weights and biases are nudged according to the backpropagation algorithm such that the cost function decreases. More specifically, the gradient of the cost function is calculated with respect to all the weights and the weights are nudged in proportion to this gradient (the same is done for the biases). Thus, the weights and biases become more and more fine tuned to the pattern recognition that is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55731f32-7a6b-4ce8-99aa-ea61955b1713",
   "metadata": {},
   "source": [
    "### The Math.\n",
    "\n",
    "Let the $L$th layer be the last layer, i.e. the output layer. Then,\n",
    "\n",
    "$$\\frac{ \\partial{C} } { \\partial{w^{(L)}_{jk}} } = \n",
    "\\frac{ \\partial{z^{(L)}_j} } { \\partial{w^{(L)}_{jk}} }\n",
    "\\frac{ \\partial{a^{(L)}_j} } { \\partial{z^{(L)}_j} }\n",
    "\\frac{ \\partial{C} } { \\partial{a^{(L)}_j} }\n",
    "$$\n",
    "\n",
    "$$z^{(L)}_j = \\sum_{n=1}^{m} a^{(L-1)}_n*w^{(L)}_{jn} + b_j$$\n",
    "\n",
    "$$\\implies \\frac{ \\partial{z^{(L)}_j} } { \\partial{w^{(L)}_{jk}} } = a^{(L-1)}_k$$\n",
    "\n",
    "$$a^{(L)}_j = \\sigma(z^{(L)}_j)$$\n",
    "\n",
    "$$\\implies \\frac{ \\partial{a^{(L)}_j} } { \\partial{z^{(L)}_j} } = \\sigma\\prime(z^{(L)}_j)$$\n",
    "\n",
    "$$C = \\frac{1}{2} \\sum_{j=1}^{N}(a^{(L)}_j-y_{j})^2$$\n",
    "\n",
    "$$\\implies \\frac{ \\partial{C} } { \\partial{a^{(L)}_j} } = a^{(L)}_j-y_{j}$$\n",
    "\n",
    "$$\\implies \\frac{ \\partial{C} } { \\partial{w^{(L)}_{jk}} } = a^{(L-1)}_k*\\sigma\\prime(z^{(L)}_j)*(a^{(L)}_j-y_{j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc299c6-c6c7-4f84-9a25-13b5244734de",
   "metadata": {},
   "source": [
    "The process of calculating $\\frac{ \\partial{C} } { \\partial{w^{(l)}_{jk}} }$ isn't as simple for an arbitrary layer $l$ that is not the output layer. Thus, to simplify notation and save some brain power, we define a new function denoted by $\\delta$ where\n",
    "\n",
    "$$\\delta^{(l)}_j = \\frac{ \\partial{C} } { \\partial{z^{(l)}_j} }$$\n",
    "\n",
    "for any arbitrary layer $l$. This new $\\delta$ covers the two last partial derivatives in the chain of partial derivatives shown above.\n",
    "\n",
    "For the output layer $L$ the following is true.\n",
    "\n",
    "$$\\delta^{(L)}_j = \\frac{ \\partial{C} } { \\partial{z^{(L)}_j} }= \\frac{ \\partial{C} }{\\partial{ a^{(L)}_j} }\\frac{ \\partial{a^{(L)}_j} } { \\partial{z^{(L)}_j} }$$\n",
    "$$= (a^{(L)}_j-y_{j})*\\sigma\\prime(z^{(L)}_j)$$ \n",
    "The above component-wise equation can be condensed using matrix multiplicaton into the following form,\n",
    "\n",
    "$$\\delta^{(L)} = (a^{(L)}-y)\\odot \\sigma\\prime(z^{(L)})$$\n",
    "\n",
    "where $\\odot$ is the hadamard operator.\n",
    "\n",
    "For any arbitrary layer and node $l$ and $j$, $\\delta^{(l)}_j$ can be computed recursively using the values of $\\delta^{(l+1)}_k$ for every $k$th node in the $l+1$th layer. To understand the following derivation, it's important to note a very relevant nontrivial fact: any node in any arbitrary layer that is not the output layer affects the cost function through multiple paths.\n",
    "<center>\n",
    "    <img src=\"diagrams/IMG_4956.png\" width=\"300\" alt=\"multiple paths\"/>\n",
    "</center>\n",
    "\n",
    "(IMG_4956)\n",
    "\n",
    "In the simplified diagram above, node $a^{(l)}_1$ indirectly affects the cost function by affecting the two nodes it directly affects -- $a^{(l+1)}_1$ and $a^{(l+1)}_2$. Thus, $\\frac{\\partial {C} }{\\partial {a^{(l)}_1} }$ can be said to be equal to \n",
    "$$\\frac{\\partial {C} }{\\partial {a^{(l+1)}_1} }\\frac{\\partial {a^{(l+1)}_1} }{\\partial {z^{(l+1)}_1} } \\frac{\\partial {z^{(l+1)}_1} }{\\partial {a^{(l)}_1} }+\n",
    "\\frac{\\partial {C} }{\\partial {a^{(l+1)}_2} }\\frac{\\partial {a^{(l+1)}_2} }{\\partial {z^{(l+1)}_2} } \\frac{\\partial {z^{(l+1)}_2} }{\\partial {a^{(l)}_1} }$$\n",
    "\n",
    "In general,\n",
    "\n",
    "$$\\frac{\\partial {C} }{\\partial {a^{(l)}_j} } $$\n",
    "$$= \\sum_{k=1}^{N}[ \\frac{\\partial {C} }{\\partial {a^{(l+1)}_k} }\\frac{\\partial {a^{(l+1)}_k} }{\\partial {z^{(l+1)}_k} } \\frac{\\partial {z^{(l+1)}_k} }{\\partial {a^{(l)}_j} }]$$\n",
    "$$= \\sum_{k=1}^{N} [\\frac{\\partial {C} }{\\partial {z^{(l+1)}_k} } \\frac{\\partial {z^{(l+1)}_k} }{\\partial {a^{(l)}_j} }]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c3401-64f5-4cb2-bdea-b323a8b8607f",
   "metadata": {},
   "source": [
    "With this in mind, computing $\\delta^{(l)}_j$ can be done using the chain rule.\n",
    "\n",
    "$$\\delta^{(l)}_j = \\frac{ \\partial{C} } { \\partial{z^{(l)}_j} }= \\frac{ \\partial{C} }{ \\partial{a^{(l)}_j} }\\frac{ \\partial{a^{(l)}_j} } { \\partial{z^{(l)}_j} } $$\n",
    "\n",
    "$$= \n",
    "\\frac{ \\partial{a^{(l)}_j} } { \\partial{z^{(l)}_j} }\n",
    "\\sum_{k=1}^{N}[\n",
    "\\frac{ \\partial{C} } { \\partial{z^{(l+1)}_k} }\n",
    "\\frac{ \\partial{z^{(l+1)}_k} }{ \\partial{a^{(l)}_j} }]$$\n",
    "\n",
    "$$\n",
    "=\\frac{ \\partial{a^{(l)}_j} } { \\partial{z^{(l)}_j} }\n",
    "\\sum_{k=1}^{N}[\n",
    "\\delta^{(l+1)}_k*\n",
    "\\frac{\\partial}{\\partial{a^{(l)}_j}}[\\sum_{z=1}^{N} a^{(l)}_z*w^{(l+1)}_{kz} + b^{(l+1)}]]\n",
    "$$\n",
    "\n",
    "$$=\\frac{ \\partial{a^{(l)}_j} } { \\partial{z^{(l)}_j} }\n",
    "\\sum_{k=1}^{N}\n",
    "\\delta^{(l+1)}_k*w^{(l+1)}_{kj}\n",
    "$$ \n",
    "\n",
    "$$=\\sigma\\prime(z^{(l)}_j)\n",
    "\\sum_{k=1}^{N}\n",
    "\\delta^{(l+1)}_k*w^{(l+1)}_{kj}\n",
    "$$\n",
    "\n",
    "In condensed matrix multiplication form, the above component-wise form can be written more simply and elegantly as\n",
    "\n",
    "$$\\delta^{(l)} = ((w^{(l+1)})^T\\delta^{(l+1)})\\odot \\sigma\\prime(z^{(l)}).$$\n",
    "\n",
    "Finally, we can come up with a general expression for any partial derivative of the cost function with respect to any weight or bias in the network. \n",
    "\n",
    "$$\\frac{\\partial {C} }{\\partial {w^{(l)}_{jk}} } = \\frac{\\partial {z^{(l)}_{j}} }{\\partial {w^{(l)}_{jk}} } \\frac{\\partial {C} }{\\partial {z^{(l)}_{j}} }$$\n",
    "$$=a^{(l-1)}_k*\\delta^{(l)}_j$$\n",
    "$$\\frac{\\partial {C} }{\\partial {b^{(l)}_j }} = \\frac{\\partial {z^{(l)}_{j}} }{\\partial {b^{(l)}_j} } \\frac{\\partial {C} }{\\partial {z^{(l)}_{j}} }$$\n",
    "$$=\\delta^{(l)}_j$$\n",
    "\n",
    "In condensed matrix form, \n",
    "\n",
    "$$\\nabla_w^{(l)} = \\delta^{(l)}A^{(l-1)T}$$\n",
    "$$\\nabla_b^{(l)} = \\delta^{(l)}$$\n",
    "\n",
    "All of this math only applies for one test case. In order to simultaneously go through all of the test cases, the gradients should be divided by the number of test cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f62be44-b21c-4032-afba-b4f6e2b067a6",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    "\n",
    "It seems best not to get caught up in the confusing matrix multiplication forms of the relatively simple component-wise equations. As I noted earlier, focusing on the dimensions of the matrices seems to be enough for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2075db4e-2035-4aba-a16d-e4540b6ed3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for test data: 0.627\n"
     ]
    }
   ],
   "source": [
    "W1,b1,W2,b2 = gradDesc(X_train,Y_train,0.1,100)\n",
    "testWB(W1,b1,W2,b2,X_dev,Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991b9e93-2c25-4637-9ee8-c9fa94b1d4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist_NN",
   "language": "python",
   "name": "mnist_nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
